{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2983a8-4cda-448f-a278-bf4e1ccebd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "llm = LLM(model=\"Qwen/Qwen2.5-7B-Instruct/\", tensor_parallel_size=4, max_model_len=8192, gpu_memory_utilization=0.85)\n",
    "df = pd.read_json(\"ScaleQM+_test.json\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct/\")\n",
    "texts = []\n",
    "for i in range(len(df)):\n",
    "    system1 = \"\"\"You are a mathematics teacher reviewing a solution that may contain logical gaps or jumps in reasoning.\n",
    "- Your task is to:\n",
    "1. Carefully examine the logical flow of the solution and identify any points where the reasoning jumps ahead without sufficient explanation or intermediate steps. For each such logical gap, specify exactly between which two consecutive steps it occurs.\n",
    "2. Provide the missing reasoning or intermediate step(s) that would bridge the gap, including any necessary explanations or equations.\n",
    "Note that the solution may contain multiple logical gaps, or it might be logically complete. The steps in the solution are labeled from Step 0 (problem statement) to Step N.\n",
    "\n",
    "- Please format your response as follows:\n",
    "For each missing step, output:\n",
    "Missing Step X:\n",
    "The missing step should be placed between Step Y and Step Y+1.\n",
    "The missing step is:\n",
    "[Write the missing step here directly]\n",
    "\n",
    "If there are no missing steps, please output:\n",
    "No missing steps.\n",
    "\n",
    "Just provide the fill locations and content according to the format, without giving extra explanations or the complete solution after filling. Don't include confirmations like \"No other missing steps\" after filling in.\n",
    "\n",
    "- Example Output:\n",
    "Missing Step 1:  \n",
    "The missing step should be placed between Step 5 and Step 6.  \n",
    "The missing step is:  \n",
    "We isolate the variable x by subtracting 5 from both sides of the equation:  \n",
    "7x + 5 = 26  \n",
    "=> 7x = 21\n",
    "\n",
    "Missing Step 2:  \n",
    "The missing step should be placed between Step 8 and Step 9.  \n",
    "The missing step is:  \n",
    "We divide both sides of the equation by 7 to solve for x:  \n",
    "7x = 21  \n",
    "=> x = 3\n",
    "\"\"\"\n",
    "    prompt1 = f\"\"\"Here is the solution.\n",
    "\n",
    "{df.iloc[i][\"messages\"][1][\"content\"][22:-24]}\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": system1},\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": prompt1},\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    texts.append(text)\n",
    "\n",
    "outputs = llm.generate(\n",
    "    texts,\n",
    "    SamplingParams(\n",
    "    temperature=0,\n",
    "    max_tokens=1024,\n",
    "    skip_special_tokens=True\n",
    ")\n",
    ")\n",
    "\n",
    "results = []\n",
    "for i in range(len(outputs)):\n",
    "    results.append(outputs[i].outputs[0].text)\n",
    "\n",
    "with open('results-7b.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
